#!/usr/bin/env python

"""Generate clustering using specific toolkit/algorithm and configuration"""
from time import time

__author__ = "Vincenzo Musco (http://www.vmusco.com)"
__date__ = "Oct 23, 2018"

import argparse
import os
import sys
import importlib
import traceback

import mlperf.clustering.toolkits
import inspect

from mlperf.clustering.clusteringtoolkit import AlreadyRanException
from mlperf.clustering.tools import read_or_draw_centroids, run_for_nr, DatasetFacts


def configure_arg_parse_optionals(parser, enforce_dataset_check=False):
    parser.add_argument('-b', '--base', type=int, action='append', help='Execute only RUN x', default=0)
    parser.add_argument('-r', '--runs', type=int, help='Number of runs to perform', default=30)
    parser.add_argument('-I', '--infinite-runs', action='store_true', help='Number of runs to perform', default=False)
    parser.add_argument('-a', '--basepath-dataset', type=str,
                        help='Specify the absolute part of the path for dataset selection', default=None)
    parser.add_argument('-d', '--dataset', action='append', required=enforce_dataset_check,
                        help='Run this dataset tsv file', default=None)
    parser.add_argument('-S', '--sep', type=str, help='Separator to use for dataset file (default \\t)', default='\t')
    parser.add_argument('-o', '--overwrite', action='store_true',
                        help='If a specific run is already ran, recompute it anyway', default=False)
    parser.add_argument('-f', '--output-path', type=str,
                        help='Overwrite the default output folder (default is same as file)', default=None)
    parser.add_argument('-s', '--reuse', action='store_true',
                        help='If several dataset, perform all runs before going to the next dataset '
                             '(no dataset interlacement).')
    parser.add_argument('-i', '--init', type=str,
                        help='Parameters needed to initialize the toolkit/variant.')
    parser.add_argument('-t', '--times', type=str, default=None,
                        help='Save generation time on a file.')


toolkit = os.listdir(mlperf.clustering.toolkits.__path__[0])
toolkit = filter(lambda x: x[0] != "_", toolkit)
toolkit = map(lambda x: x[:-3], toolkit)
toolkit = list(toolkit)

parser = argparse.ArgumentParser(description='Generate clusters for dataset', add_help=False)
parser.add_argument("toolkit", choices=toolkit)
parser.add_argument('variant', nargs='?', help="toolkit specific")
parser.add_argument('algorithm', nargs='?', help="toolkit/variant specific")
parser.add_argument('--help', action='store_true')
configure_arg_parse_optionals(parser)

args = parser.parse_args()
toolkit = args.toolkit

i = importlib.import_module("{}.{}".format(mlperf.clustering.toolkits.__package__, toolkit))

## Toolkit Variant
argument_position = sys.argv.index(toolkit)
argument_value = sys.argv.pop(argument_position)
sys.argv[0] = "{} {}".format(sys.argv[0], argument_value)

available_classes = list(filter(lambda x: inspect.isclass(getattr(i, x)), dir(i)))
available_variants_algorithms = {}

for a_class in available_classes:
    the_class_info = getattr(i, a_class)

    functions = inspect.getmembers(the_class_info, inspect.isfunction)
    functions = list(filter(lambda x: x[0][0:4] == "run_", functions))

    if len(functions) > 0:
        available_variants_algorithms[a_class] = list(map(lambda x: x[0][4:], functions))

parser = argparse.ArgumentParser(description='Generate clusters for dataset', add_help=False)
parser.add_argument("variant", choices=available_variants_algorithms.keys(), help="Variant to run")
parser.add_argument('algorithm', nargs='?', help="variant specific")
parser.add_argument('--help', action='store_true')
configure_arg_parse_optionals(parser)
args = parser.parse_args()
variant = args.variant

## Toolkit Algorithm
argument_position = sys.argv.index(variant)
argument_value = sys.argv.pop(argument_position)
sys.argv[0] = "{} {}".format(sys.argv[0], argument_value)

parser = argparse.ArgumentParser(description='Generate clusters for dataset')
parser.add_argument('algorithm', choices=available_variants_algorithms[variant], help="Algorithm to run")
configure_arg_parse_optionals(parser, True)
args = parser.parse_args()
algorithm = args.algorithm

candidate_class = getattr(i, variant)
init = {}
if args.init:
    for init_option in args.init.split(";"):
        init_key, init_value = init_option.split("=")
        init[init_key] = init_value

print("Loading module {}/{}...".format(toolkit, variant), file=sys.stderr)
try:
    class_instance = candidate_class(**init)
    print("✔ Module {}/{} loaded successfully.".format(toolkit, variant), file=sys.stderr)
except Exception as e:
    print("⨯ Error importing module {}/{}. Please check your system installation.".format(toolkit, variant),
          file=sys.stderr)
    print("\t -> {}".format(e), file=sys.stderr)
    sys.exit(1)

class_instance.set_overwrite_ran_iterations(args.overwrite)

if args.output_path is not None:
    class_instance.set_redirect_output_path(args.output_path)

    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)

candidate_function = getattr(candidate_class, "run_{}".format(algorithm))
function_instance = getattr(class_instance, "run_{}".format(algorithm))

## Running Logic

reuse = args.reuse
if len(args.dataset) == 1:
    reuse = True

print("\nCurrent configuration:")
print("- Toolkit = {}".format(toolkit))
print("- Variant = {}".format(variant))
print("- Algorithm = {}".format(algorithm))
print("- Runs = {}".format(args.runs if not args.infinite_runs else "Infinite"))
print("- Dataset:")
for dataset in args.dataset:
    print("\t- {}".format(dataset))
print("- Base = {}".format(args.base))
print("- Reuse = {}".format(reuse))
print("- Separator = {}".format(repr(args.sep)))

fulldatasets = args.dataset

if args.basepath_dataset:
    fulldatasets = list(map(lambda x: "{}/{}".format(args.basepath_dataset, x), fulldatasets))

if reuse:
    fulldatasets = list(map(lambda x: [x], fulldatasets))
else:
    fulldatasets = [fulldatasets]

for datasets in fulldatasets:
    data = None
    ground_truth_cluster_ids = None
    number_clusters = None
    data_without_target = None

    runid = 0
    while args.infinite_runs or runid < args.runs:
        resolved_run_nr = args.base + runid
        RUN_INFO = run_for_nr(class_instance.toolkit_name(), algorithm, resolved_run_nr)
        print("\n* RUN: {}.".format(RUN_INFO))

        for dataset in datasets:
            run_infos = "RUN {}. ALGO = {}".format(runid, algorithm)
            print("\t{}".format(run_infos), end="")
            sys.stdout.flush()
            source_file = dataset

            if data is None or not reuse:
                # print("Reading file {}...".format(source_file))
                df = DatasetFacts.read_dataset(source_file, args.sep)
                data = df.data
                number_clusters = df.nb_clusters()
                ground_truth_cluster_ids = df.ground_truth_cluster_ids()
                data_without_target = df.data_without_target()
                print(" [loaded data: {} features, {} instances, {} clusters]".format(df.nb_features(),
                                                                                      df.nb_instances(),
                                                                                      number_clusters),
                      end="")
            else:
                print(" [reusing data]", end="")

            try:
                start_time = time()
                if algorithm == 'ap':
                    # AP Cluster does not expect a #clusters
                    function_instance(source_file, data_without_target, dataset, resolved_run_nr, RUN_INFO)
                    pass
                elif algorithm == 'kmeans':
                    # Kmeans requires centroid input !
                    centroids_file, centroids = read_or_draw_centroids(source_file, runid, number_clusters,
                                                                       data_without_target, args.output_path)
                    function_instance(number_clusters, source_file, data_without_target, dataset, centroids_file,
                                      centroids, resolved_run_nr, RUN_INFO)
                else:
                    function_instance(number_clusters, source_file, data_without_target, dataset, resolved_run_nr,
                                      RUN_INFO)

                if args.times is not None:
                    with open(args.times, "a") as times_fp:
                        times_fp.write("{} = {}\n".format(run_infos, (time() - start_time)))

            except Exception as e:
                print("!!!!! Exception occured !!!!!")
                print(sys.exc_info())
                print(e)
                traceback.print_exc()
            except AlreadyRanException as e:
                print(" [skipped]", end="")

        runid += 1
